{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-429715",
   "metadata": {},
   "source": [
    "# McDonald's Reviews - Model Training and Comparison\n",
    "\n",
    "**Step 2**: Train classical ML models and neural network, compare performance on noisy reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-86207",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-369707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-259790",
   "metadata": {},
   "source": [
    "### Import Neural Network Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-228526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:51:44.330105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# For RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-886490",
   "metadata": {},
   "source": [
    "## 2. Load Data with Noise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-607311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA WITH NOISE ANALYSIS\n",
      "================================================================================\n",
      "Loaded: 33,396 reviews\n",
      "Noisy reviews: 18,584 (55.6%)\n",
      "Clean reviews: 14,812 (44.4%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA WITH NOISE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = pd.read_csv('data_with_noise_analysis.csv')\n",
    "\n",
    "print(f\"Loaded: {len(df):,} reviews\")\n",
    "print(f\"Noisy reviews: {df['has_noise'].sum():,} ({df['has_noise'].sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"Clean reviews: {(~df['has_noise']).sum():,} ({(~df['has_noise']).sum()/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-696177",
   "metadata": {},
   "source": [
    "## 3. Minimal Preprocessing\n",
    "\n",
    "**Important**: Preserving noise for robustness testing. Only removing URLs and normalizing whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-843978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MINIMAL PREPROCESSING (PRESERVING NOISE FOR ROBUSTNESS TEST)\n",
      "================================================================================\n",
      "✓ Minimal preprocessing applied\n",
      "✓ Noise preserved: typos, slang, abbreviations kept intact\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MINIMAL PREPROCESSING (PRESERVING NOISE FOR ROBUSTNESS TEST)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def minimal_clean(text):\n",
    "    \"\"\"\n",
    "    Minimal cleaning - preserve most noise!\n",
    "    Only remove URLs and extreme cases, keep typos/slang/abbreviations\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['review_clean'] = df['review'].apply(minimal_clean)\n",
    "\n",
    "print(\"✓ Minimal preprocessing applied\")\n",
    "print(\"✓ Noise preserved: typos, slang, abbreviations kept intact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-777443",
   "metadata": {},
   "source": [
    "### Preview Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-720807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples:\n",
      "\n",
      "Original: Why does it look like someone spit on my food?\n",
      "I had a normal transaction,  everyone was chill and p\n",
      "Cleaned:  why does it look like someone spit on my food? i had a normal transaction, everyone was chill and po\n",
      "\n",
      "Original: It'd McDonalds. It is what it is as far as the food and atmosphere go. The staff here does make a di\n",
      "Cleaned:  it'd mcdonalds. it is what it is as far as the food and atmosphere go. the staff here does make a di\n",
      "\n",
      "Original: Made a mobile order got to the speaker and checked it in.\n",
      "Line was not moving so I had to leave othe\n",
      "Cleaned:  made a mobile order got to the speaker and checked it in. line was not moving so i had to leave othe\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExamples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df.iloc[i]['review'][:100]}\")\n",
    "    print(f\"Cleaned:  {df.iloc[i]['review_clean'][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-847738",
   "metadata": {},
   "source": [
    "## 4. Data Splitting (Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-937072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA SPLITTING (STRATIFIED)\n",
      "================================================================================\n",
      "Train set: 20,037 samples (60%)\n",
      "Val set:   6,679 samples (20%)\n",
      "Test set:  6,680 samples (20%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA SPLITTING (STRATIFIED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X = df['review_clean']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 20% val, 20% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train):,} samples (60%)\")\n",
    "print(f\"Val set:   {len(X_val):,} samples (20%)\")\n",
    "print(f\"Test set:  {len(X_test):,} samples (20%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-108387",
   "metadata": {},
   "source": [
    "### Verify Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-593853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "Train: Neg=37.5% Neu=14.4% Pos=48.1%\n",
      "Val  : Neg=37.5% Neu=14.4% Pos=48.1%\n",
      "Test : Neg=37.5% Neu=14.4% Pos=48.1%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClass distribution:\")\n",
    "for split_name, split_y in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "    dist = split_y.value_counts(normalize=True) * 100\n",
    "    print(f\"{split_name:5s}: Neg={dist['negative']:.1f}% Neu={dist['neutral']:.1f}% Pos={dist['positive']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-705837",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-917940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TF-IDF VECTORIZATION\n",
      "================================================================================\n",
      "✓ TF-IDF features created\n",
      "  Vocabulary size: 5,000\n",
      "  Feature matrix shape: (20037, 5000)\n",
      "  Sparsity: 99.5%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TF-IDF VECTORIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"✓ TF-IDF features created\")\n",
    "print(f\"  Vocabulary size: {len(tfidf.vocabulary_):,}\")\n",
    "print(f\"  Feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"  Sparsity: {(1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1]))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-637085",
   "metadata": {},
   "source": [
    "## 6. Train Classical ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-779073",
   "metadata": {},
   "source": [
    "### 6.1 Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-378882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING CLASSICAL ML MODELS\n",
      "================================================================================\n",
      "\n",
      "1. Baseline (Majority Class): 0.4809\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING CLASSICAL ML MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Baseline: Majority class\n",
    "baseline_class = y_train.value_counts().idxmax()\n",
    "baseline_pred = [baseline_class] * len(y_val)\n",
    "baseline_acc = accuracy_score(y_val, baseline_pred)\n",
    "print(f\"\\n1. Baseline (Majority Class): {baseline_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-394992",
   "metadata": {},
   "source": [
    "### 6.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-831489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Training Logistic Regression...\n",
      "   Validation Accuracy: 0.8169\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. Training Logistic Regression...\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "lr_pred = lr.predict(X_val_tfidf)\n",
    "lr_acc = accuracy_score(y_val, lr_pred)\n",
    "print(f\"   Validation Accuracy: {lr_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-254837",
   "metadata": {},
   "source": [
    "### 6.3 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-471541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Training SVM (LinearSVC)...\n",
      "   Validation Accuracy: 0.8155\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "print(\"\\n3. Training SVM (LinearSVC)...\")\n",
    "svm = LinearSVC(max_iter=10000, random_state=42)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "svm_pred = svm.predict(X_val_tfidf)\n",
    "svm_acc = accuracy_score(y_val, svm_pred)\n",
    "print(f\"   Validation Accuracy: {svm_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-477806",
   "metadata": {},
   "source": [
    "### 6.4 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-478654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Training Random Forest...\n",
      "   Validation Accuracy: 0.7813\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print(\"\\n4. Training Random Forest...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=30, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "rf_pred = rf.predict(X_val_tfidf)\n",
    "rf_acc = accuracy_score(y_val, rf_pred)\n",
    "print(f\"   Validation Accuracy: {rf_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-889787",
   "metadata": {},
   "source": [
    "## 7. Train Neural Network (RNN-LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-583955",
   "metadata": {},
   "source": [
    "### 7.1 Prepare Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-845443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING NEURAL NETWORK (RNN-LSTM)\n",
      "================================================================================\n",
      "Tokenizing text for RNN...\n",
      "✓ Sequences created (max length: 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING NEURAL NETWORK (RNN-LSTM)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tokenization for RNN\n",
    "print(\"Tokenizing text for RNN...\")\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "maxlen = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=maxlen, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post')\n",
    "\n",
    "print(f\"✓ Sequences created (max length: {maxlen})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-869393",
   "metadata": {},
   "source": [
    "### 7.2 Define RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-924116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN model\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, embedding_dim=128, hidden_dim=64, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SentimentRNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-344853",
   "metadata": {},
   "source": [
    "### 7.3 Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-733345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "y_train_enc = y_train.map(label_map).values\n",
    "y_val_enc = y_val.map(label_map).values\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(X_train_pad),\n",
    "    torch.LongTensor(y_train_enc)\n",
    ")\n",
    "val_data = torch.utils.data.TensorDataset(\n",
    "    torch.LongTensor(X_val_pad),\n",
    "    torch.LongTensor(y_val_enc)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-362661",
   "metadata": {},
   "source": [
    "### 7.4 Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-862158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RNN...\n",
      "  Epoch 1/5, Loss: 0.9968\n",
      "  Epoch 2/5, Loss: 0.9848\n",
      "  Epoch 3/5, Loss: 0.8777\n",
      "  Epoch 4/5, Loss: 0.7785\n",
      "  Epoch 5/5, Loss: 0.7479\n"
     ]
    }
   ],
   "source": [
    "# Train RNN\n",
    "print(\"\\nTraining RNN...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"  Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0df65b-e8ea-43f9-be4b-c5793c0c5952",
   "metadata": {},
   "source": [
    "NO early stopping\n",
    "NO gradient clipping\n",
    "Just 5 epochs of basic training\n",
    "This RNN will fail on neutral class (F1 = 0.00) the 3rd notebook of mine, I will work with improved RNN model with early stopping and class weightining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-432949",
   "metadata": {},
   "source": [
    "### 7.5 Evaluate RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3952fca-afdc-404e-9ac1-94d27e322c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-854074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. RNN (LSTM) Validation Accuracy: 0.6857\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "rnn_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, _ in val_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        rnn_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "rnn_acc = accuracy_score(y_val_enc, rnn_predictions)\n",
    "print(f\"\\n5. RNN (LSTM) Validation Accuracy: {rnn_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-756106",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-516223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON (VALIDATION SET)\n",
      "================================================================================\n",
      "\n",
      "              Model  Accuracy\n",
      "Logistic Regression  0.816889\n",
      "                SVM  0.815541\n",
      "      Random Forest  0.781255\n",
      "           RNN-LSTM  0.685731\n",
      "           Baseline  0.480910\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON (VALIDATION SET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'SVM', 'Random Forest', 'RNN-LSTM', 'Baseline'],\n",
    "    'Accuracy': [lr_acc, svm_acc, rf_acc, rnn_acc, baseline_acc]\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-204464",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-820055",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['blue', 'green', 'orange', 'purple', 'red']\n",
    "bars = plt.bar(results['Model'], results['Accuracy'], color=colors)\n",
    "plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "plt.title('Model Performance Comparison on Noisy Reviews', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, val in zip(bars, results['Accuracy']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, val + 0.02, \n",
    "             f'{val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_validation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Saved: model_comparison_validation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-397705",
   "metadata": {},
   "source": [
    "## 9. Save Models and Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-366556",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING MODELS AND PREPARED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save models\n",
    "joblib.dump(lr, 'model_logistic_regression.pkl')\n",
    "joblib.dump(svm, 'model_svm.pkl')\n",
    "joblib.dump(rf, 'model_random_forest.pkl')\n",
    "torch.save(model.state_dict(), 'model_rnn_lstm.pth')\n",
    "\n",
    "# Save vectorizer and tokenizer\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(tokenizer, 'rnn_tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-386273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits with noise annotations\n",
    "test_df = pd.DataFrame({\n",
    "    'review': X_test.values,\n",
    "    'sentiment': y_test.values,\n",
    "    'review_index': X_test.index\n",
    "})\n",
    "test_df = test_df.merge(df[['total_noise', 'noise_level', 'has_noise']], \n",
    "                        left_on='review_index', right_index=True)\n",
    "test_df.to_csv('test_set_with_noise.csv', index=False)\n",
    "\n",
    "print(\"✓ Saved all models\")\n",
    "print(\"✓ Saved vectorizer and tokenizer\")\n",
    "print(\"✓ Saved test set with noise annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-218541",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-829151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STEP 2 COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\"\"\n",
    "Models trained:\n",
    "- Logistic Regression: {lr_acc:.4f}\n",
    "- SVM (LinearSVC): {svm_acc:.4f} ← BEST\n",
    "- Random Forest: {rf_acc:.4f}\n",
    "- RNN-LSTM: {rnn_acc:.4f}\n",
    "\n",
    "Ready for noise robustness analysis in Step 3!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
